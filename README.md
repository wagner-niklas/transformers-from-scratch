# transformers-from-scratch

This repository is based on a tutorial taught at KIT that explains transformers step by step, including self-attention and multi-head attention.

The original code has been adapted to run on Apple Silicon Macs using MPS, to experiment with training a character-level transformer on your own texts without an NVIDIA GPU.

This is purely an educational resource for learning and experimentation. All modifications are for Mac compatibility.