{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6826b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available: True\n",
      "CUDA available: False\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niklaswagner/Developer/transformers-from-scratch/.venv/lib/python3.14/site-packages/torch/_subclasses/functional_tensor.py:283: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) is available\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "\n",
    "# Check if CUDA is available (it won't be on Mac)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() \n",
    "    else \"mps\" if torch.backends.mps.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b989f791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape was torch.Size([1, 10, 800]), output shape should be identical torch.Size([1, 10, 800])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Input and output dimension of each input vector.\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    scale : float\n",
    "        Normalizing constant for the dot product.\n",
    "    qkv : nn.Linear\n",
    "        Linear projection for the query, key and value.\n",
    "    proj : nn.Linear\n",
    "        Linear mapping that takes in the concatenated output of all attention\n",
    "        heads and maps it into a new space.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads=12, masked_attention=False, context_size=None):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "\n",
    "        #  We create a linear layer that takes a vector which contains\n",
    "        #  d input elements and create a resulting vector with size d*3\n",
    "        #  in which we store k, q and v.\n",
    "\n",
    "        #  This is identical to:\n",
    "        #  k = nn.Linear(dim, dim)\n",
    "        #  q = nn.Linear(dim, dim)\n",
    "        #  v = nn.Linear(dim, dim)\n",
    "        #  self.qkv = torch.cat(q,k,v dim=-1)\n",
    "\n",
    "\n",
    "        # last output layer to aggregate the information of the merged heads\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        # for masked/causal attention\n",
    "        self.masked_attention = masked_attention\n",
    "        if masked_attention:\n",
    "            self.mask = torch.tril(torch.ones(context_size, context_size)) \\\n",
    "                            .view(1, 1, context_size, context_size) \\\n",
    "                            .to(device)            # sets 1 if attention is allowed, 0 otherwise.\n",
    "            # 10000\n",
    "            # 11000\n",
    "            # 11100\n",
    "            # 11110\n",
    "            # 11111\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(batchsize, number of input vectors, dim)`.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(batchsize, number of input vectors, dim)`.\n",
    "        \"\"\"\n",
    "        batchsize, n_input_vectors, dim = x.shape\n",
    "\n",
    "        # if linear layer receives input with three dimension, its weight matrix\n",
    "        # is duplicated for each vector\n",
    "        qkv = self.qkv(x)  # output has dimension (batchsize, number input vectors, 3 * dim)\n",
    "\n",
    "\n",
    "        # qkv calculates for all input vectors their key query and value vectors\n",
    "        # We need to \"cut out\" the queries, keys and values\n",
    "        # Simultaneously we also \"cut out\" the heads for multihead attention\n",
    "        qkv = qkv.reshape(\n",
    "                batchsize, n_input_vectors, 3, self.n_heads, self.head_dim\n",
    "        )  # (batchsize, number input vectors, 3, n_heads, head_dim)\n",
    "\n",
    "\n",
    "        # Later we will do matrix multiplication with 4D-tensors\n",
    "        # We intentionally skipped this part in lecture but we need to\n",
    "        # change the order of the dimensions so we can calculate the dot product\n",
    "        # of all heads at the same time\n",
    "        qkv = qkv.permute(\n",
    "                2, 0, 3, 1, 4\n",
    "        )  # (3, batchsize, n_heads, number input vectors, head_dim)\n",
    "\n",
    "\n",
    "        # q contains the queries of all heads of all input vectors\n",
    "        # same for k and v\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # transpose the last two dimension of keys\n",
    "        k_t = k.transpose(-2, -1)  # (batchsize, n_heads, head_dim, number input vectors)\n",
    "\n",
    "\n",
    "        # @ is the symbol for matrix multiplication\n",
    "        # if you want to know how 4d matmul works: https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
    "        # self.scale = 1 / root(head_dim)\n",
    "        s_dot_product = (q @ k_t) * self.scale # (batchsize, n_heads, number input vectors, number input vectors)\n",
    "\n",
    "\n",
    "        # Only used in Decoders that use masked attention, This sets all values to -inf if i > j\n",
    "        if self.masked_attention:\n",
    "            s_dot_product = s_dot_product.masked_fill(self.mask[:,:,:n_input_vectors, :n_input_vectors]== 0, float('-inf'))\n",
    "\n",
    "\n",
    "        attn = s_dot_product.softmax(dim=-1)  # (batchsize, n_heads, number input vectors, number input vectors)\n",
    "\n",
    "        weighted_avg = attn @ v  # (batchsize, n_heads, number input vectors, head_dim)\n",
    "\n",
    "\n",
    "        # reverse order of number input vectors and n_heads ()\n",
    "        weighted_avg = weighted_avg.transpose(\n",
    "                1, 2\n",
    "        )  # (batchsize, number input vectors, n_heads, head_dim)\n",
    "\n",
    "        # Merge the heads back into one input vector\n",
    "        weighted_avg = weighted_avg.flatten(2)  # (batchsize, number input vectors, dim)\n",
    "\n",
    "\n",
    "        # Final output layer to aggreagte information of heads\n",
    "        x = self.proj(weighted_avg)  # (batchsize, number input vectors, dim)\n",
    "\n",
    "        return x\n",
    "\n",
    "## create 10 input vectors each with size 800.\n",
    "dummy_input = torch.rand(1,10, 800)\n",
    "\n",
    "## create Multihead Attention Block\n",
    "mha = Attention(dim=800, n_heads=8)\n",
    "\n",
    "## feed dummy input through multihead attention\n",
    "dummy_result = mha(dummy_input)\n",
    "print(f\"input shape was {dummy_input.shape}, output shape should be identical {dummy_result.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "657c08c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape was torch.Size([1, 10, 800]), output shape should be identical torch.Size([1, 10, 800])\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Multilayer perceptron.\n",
    "\n",
    "    Called Fully Connected Layers in lecture  (blue block inside the transformer block)\n",
    "\n",
    "    Contains two fully connected layers.\n",
    "        - the first fully connected layer quadruples vector size\n",
    "        - the second fully connected layer decreases vector size back to original input size\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Size of vectors.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    fc : nn.Linear\n",
    "        The First linear layer.\n",
    "    act : nn.GELU\n",
    "        GELU activation function.\n",
    "    fc2 : nn.Linear\n",
    "        The second linear layer.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=dim, out_features=4*dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(in_features=4*dim, out_features=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(batchsize, number input vectors, in_features)`.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(batchsize, number input vectors, in_features)`\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)  # (batchsize, number input vectors, 4*dim)\n",
    "        x = self.act(x)  # (batchsize, number input vectors, 4*dim)\n",
    "        x = self.fc2(x)  # (batchsize, number input vectors, dim)\n",
    "        return x\n",
    "\n",
    "## create 10 input vectors each with size 800.\n",
    "dummy_input = torch.rand(1,10, 800)\n",
    "\n",
    "## create MLP Block\n",
    "FC = MLP(dim=800)\n",
    "\n",
    "## feed dummy input through fully connected layers\n",
    "dummy_result = FC(dummy_input)\n",
    "print(f\"input shape was {dummy_input.shape}, output shape should be identical {dummy_result.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6901dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape was torch.Size([1, 10, 800]), output shape should be identical torch.Size([1, 10, 800])\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Embedding dimension.\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    norm1, norm2 : LayerNorm\n",
    "        Layer normalization.\n",
    "    attn : Attention\n",
    "        Attention module.\n",
    "    mlp : MLP\n",
    "        MLP module.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads, is_decoder=False, context_size=None):\n",
    "        super().__init__()\n",
    "        ## Layer norm is (a-µ)/sigma\n",
    "        ## it is possible that the standard deviation == 0 so we always add a tiny value (eps)\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "\n",
    "        # Multihead attention (can be masked if decoder)\n",
    "        self.attn = Attention(\n",
    "                dim,\n",
    "                n_heads=n_heads,\n",
    "                masked_attention = is_decoder,\n",
    "                context_size=context_size\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "\n",
    "        # the two fully connected layers\n",
    "        self.mlp = MLP(\n",
    "                dim=dim,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(batchsize, number input vectors, dim)`.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(batchsize, number input vectors, dim)`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Two skip connections\n",
    "        # We use pre-normalization here and normalize at the beginning\n",
    "        # instead of after the attention/mlp part\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "## create 10 input vectors each with size 800.\n",
    "dummy_input = torch.rand(1,10, 800)\n",
    "\n",
    "## create transformer block\n",
    "transformer_block_1 = Block(dim=800, n_heads=4)\n",
    "## and lets create another transformer Block\n",
    "transformer_block_2 = Block(dim=800, n_heads=8)\n",
    "\n",
    "## feed dummy input through both stacked transformer blocks\n",
    "dummy_result = transformer_block_2(transformer_block_1(dummy_input))\n",
    "\n",
    "print(f\"input shape was {dummy_input.shape}, output shape should be identical {dummy_result.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10401751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Split image into patches and then embed them.\n",
    "    Parameters\n",
    "    ----------\n",
    "    patch_size : int\n",
    "        Size of the patch (it is a square).\n",
    "    embed_dim : int\n",
    "        The emmbedding dimension. --> how many values should our input vector have\n",
    "    Attributes\n",
    "    ----------\n",
    "    proj : nn.Conv2d\n",
    "        Convolutional layer that does both the splitting into patches\n",
    "        and their embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        \"\"\"\n",
    "        Notice: Stride == kernel size\n",
    "        This cuts out a patch and then uses the convolutional Layer to create a d-size vector\n",
    "        \"\"\"\n",
    "        self.proj = nn.Conv2d(\n",
    "                in_channels=3, # 3 input channels because image is RGB\n",
    "                out_channels=embed_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(batchsize, in_chans, img_size, img_size)`.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(batchsize, n_patches, embed_dim)`.\n",
    "        \"\"\"\n",
    "        x = self.proj(x)  # (batchsize, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
    "\n",
    "        \"\"\"\n",
    "        Create a 1-D vector of each patch\n",
    "        \"\"\"\n",
    "        x = x.flatten(2)  # (batchsize, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)  # (batchsize, n_patches, embed_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "119d99f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from urllib.request import urlopen\n",
    "from tqdm import tqdm\n",
    "\n",
    "class VisionAndLanguageTransformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            context_size= 384,\n",
    "            patch_size=16,\n",
    "            n_classes=1000,\n",
    "            embed_dim=768,\n",
    "            depth=12,\n",
    "            n_heads=12,\n",
    "            is_ViT = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_ViT = is_ViT\n",
    "        self.is_LM = not is_ViT\n",
    "\n",
    "\n",
    "        if self.is_ViT:\n",
    "            self.patch_embed = PatchEmbed(\n",
    "                    patch_size=patch_size,\n",
    "                    embed_dim=embed_dim,\n",
    "            )\n",
    "\n",
    "            # create a zeroth vector with the same size as the patch vectors\n",
    "            # but with trainable values\n",
    "            # initialize with zeros\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "            # create positional embedding\n",
    "            # start with zeros and learn what they should be\n",
    "            n_patches = (context_size // patch_size)**2\n",
    "            self.context_size = n_patches + 1 # each patch + one cls token\n",
    "\n",
    "        if self.is_LM:\n",
    "                # we predict each token, so that is our vocab size\n",
    "                self.token_embed = nn.Embedding(n_classes, embed_dim)\n",
    "                self.context_size = context_size\n",
    "\n",
    "        self.pos_embed = nn.Parameter(\n",
    "                torch.zeros(1, context_size, embed_dim)\n",
    "        )\n",
    "\n",
    "        # stack as many blocks as defined in input \"depth\"\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=embed_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    is_decoder=self.is_LM,\n",
    "                    context_size=context_size\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # one final layer norm at the end of the transformer\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "\n",
    "        # final fully connected layer that takes the first output vector\n",
    "        # and predicts values for all classes\n",
    "        self.head = nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"Run the forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(batchsize, in_chans, img_size, img_size)`.\n",
    "        Returns\n",
    "        -------\n",
    "        logits : torch.Tensor\n",
    "            Logits over all the classes - `(batchsize, n_classes)`.\n",
    "        \"\"\"\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        # create patches from input image\n",
    "\n",
    "        if self.is_ViT:\n",
    "            x = self.patch_embed(x)\n",
    "\n",
    "            # only necessary because we train with batches\n",
    "            cls_token = self.cls_token.expand(\n",
    "                    batchsize, -1, -1\n",
    "            )  # (batchsize, 1, embed_dim)\n",
    "\n",
    "            # put the the class token vector on the zeroth position\n",
    "            x = torch.cat((cls_token, x), dim=1)  # (batchsize, 1 + n_patches, embed_dim)\n",
    "\n",
    "        if self.is_LM:\n",
    "            # (batchsize, n, embed_dim)\n",
    "            x = self.token_embed(x)\n",
    "\n",
    "        # add the positional encoding ontop of our input values\n",
    "\n",
    "        number_tokens = x.shape[1]\n",
    "        x = x + self.pos_embed[:,:number_tokens,:]  # (batchsize, 1 + n_patches, embed_dim)\n",
    "\n",
    "        # feed input through all transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Final normalization of the output\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if self.is_ViT:\n",
    "            # take the vector that is on the zeroth position\n",
    "            x = x[:, 0]\n",
    "        else:\n",
    "            # for generation we will take the last vector\n",
    "            # but in pretraining we will use all vectors\n",
    "            pass\n",
    "\n",
    "\n",
    "        # feed it throuh a fully connected layer\n",
    "        x = self.head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(x.view(-1, x.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return x, loss\n",
    "\n",
    "        return x\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        while prompt.shape[1] < self.context_size:\n",
    "            logits = self(prompt)\n",
    "            # take vector furthest right\n",
    "            logits = logits[:,-1,:] / 0.8 # use 0.8 temperature\n",
    "            # Top-K decoding take the 20 most likely predictions\n",
    "            v, _ = torch.topk(logits, 20)\n",
    "\n",
    "            # set prediction for all other logits to -inf\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # sample from distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # append token to prompt\n",
    "\n",
    "            prompt = torch.cat((prompt, next_token), dim=1)\n",
    "\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6559d0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of our dataset: \n",
      " I. A SCANDAL IN BOHEMIA\n",
      "\n",
      "\n",
      "I.\n",
      "\n",
      "To Sherlock Holmes she is always _the_ woman. I have seldom heard him\n",
      "mention her under any other name. In his eyes she eclipses and\n",
      "predominates the whole of her sex. It was not that he felt any emotion\n",
      "akin to love for Irene Adler. All emotions, and that one particularly,\n",
      "were abhorrent to his cold, precise but admirably balanced mind. He\n",
      "was, I take it, the most perfect reasoning and observing machine that\n",
      "the world has seen, but as a lover he would h\n",
      "\n",
      "Convert these letters into tokens  ['\\n', '\\r', ' ', '!', '#', '$', '%', '&', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', '½', 'à', 'â', 'æ', 'è', 'é', 'œ', '—', '‘', '’', '“', '”', '•', '™', '\\ufeff']\n",
      "\n",
      "The text contains 99 characters which we will use as vocabulary\n"
     ]
    }
   ],
   "source": [
    "# We won't use BPE so each symbol in text is a token\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, english=True):\n",
    "        # We can use the books of shakespeare for training\n",
    "        # or books from german authors\n",
    "        if english:\n",
    "            resource_shakespeare  = urlopen(\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
    "            self.train_text =  resource_shakespeare.read().decode('utf-8')\n",
    "            print(f\"dataset has {len(self.train_text)} characters\")\n",
    "            print(\"random part of shakespeare \\n\",self.train_text[314825:315270])\n",
    "\n",
    "        else:\n",
    "            urls = [ \n",
    "                    # The Adventures of Sherlock Holmes\n",
    "                    \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
    "                    # GÖTHE\n",
    "                    #\"https://gutenberg.org/files/2229/2229-0.txt\", # Faust 1\n",
    "                    #\"https://www.gutenberg.org/cache/epub/2407/pg2407.txt\", # Leiden des Werther\n",
    "                    #\"https://www.gutenberg.org/cache/epub/2230/pg2230.txt\",  # Faust 2\n",
    "                    #\"https://www.gutenberg.org/cache/epub/2321/pg2321.txt\", # Götz von Berlichingen\n",
    "                    #\"https://www.gutenberg.org/cache/epub/10425/pg10425.txt\", # Torquato Tasso\n",
    "                    #\"https://www.gutenberg.org/cache/epub/2054/pg2054.txt\", # Iphigenie auf Tauris\n",
    "                    #\"https://www.gutenberg.org/cache/epub/8565/pg8565.txt\", # Viele deutsche Gedichte inkl. Erlkönig\n",
    "                    # Schiller\n",
    "                    #\"https://www.gutenberg.org/cache/epub/47804/pg47804.txt\", # Die Räuber\n",
    "                    #\"https://www.gutenberg.org/cache/epub/6498/pg6498.txt\", # Kabale und Liebe\n",
    "                    #\"https://www.gutenberg.org/files/6518/6518-0.txt\", # Wallensteins Lager\n",
    "                    #\"https://www.gutenberg.org/cache/epub/6525/pg6525.txt\", # Piccolomini\n",
    "                    #\"https://www.gutenberg.org/cache/epub/6549/pg6549.txt\", # Wallensteins Tod\n",
    "                    ]\n",
    "            self.train_text = \"\"\n",
    "\n",
    "\n",
    "            for url in urls:\n",
    "                book = urlopen(url)\n",
    "                book = book.read().decode('utf-8')\n",
    "                self.train_text += book\n",
    "\n",
    "            # Holmes part\n",
    "            print(\"Part of our dataset: \\n\",self.train_text[1501:2000])\n",
    "            # Faust part\n",
    "            #print(\"Part of our dataset: \\n\",self.train_text[16701:17073])\n",
    "\n",
    "        letters = set(self.train_text)\n",
    "        letters = sorted(letters)\n",
    "\n",
    "        print(\"\\nConvert these letters into tokens \", letters)\n",
    "        self.vocab_size = len(letters)\n",
    "        print(f\"\\nThe text contains {self.vocab_size} characters which we will use as vocabulary\")\n",
    "\n",
    "\n",
    "        self.letter_to_token = {k: v for v, k in enumerate(list(letters))}\n",
    "        self.token_to_letter = {v: k for v, k in enumerate(list(letters))}\n",
    "\n",
    "    def text_to_tokens(self, text):\n",
    "\n",
    "        tokens = []\n",
    "        for i in text:\n",
    "            tokens.append(self.letter_to_token[i])\n",
    "        return torch.LongTensor(tokens)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # always return 256 tokens\n",
    "\n",
    "        input = self.text_to_tokens(self.train_text[idx:idx+256])\n",
    "        label = self.text_to_tokens(self.train_text[idx+1:idx+256+1])\n",
    "        return input, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_text) - 256 - 1\n",
    "\n",
    "dataset = TextDataset(english=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b926c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your model and move it to the chosen device\n",
    "gpt = VisionAndLanguageTransformer(\n",
    "    context_size=256,   # Sequence length: how many characters the model sees at once\n",
    "                        # Shorter sequences = less memory usage, faster training\n",
    "    n_classes=dataset.vocab_size,  # Number of possible output tokens (characters)\n",
    "    embed_dim=384,      # Embedding dimension: size of hidden representation for each token\n",
    "                        # Smaller = fits on Mac RAM; larger = more expressive but slower\n",
    "    is_ViT=False,       # Whether to use Vision Transformer-style patching (not needed for text-only)\n",
    "    depth=3             # Number of transformer blocks/layers\n",
    ").to(device)\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=128, # Changed batch size from 256 to 128\n",
    "                        shuffle=True, num_workers=0)  # use 0 on Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8385cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text, model, dataset):\n",
    "    device = next(model.parameters()).device  # get the device of the model\n",
    "    model.eval()\n",
    "    \n",
    "    def token_to_text(tokens):\n",
    "        if tokens.ndim > 1:\n",
    "            tokens = tokens[0]  # take first batch\n",
    "        return [dataset.token_to_letter[int(t)] for t in tokens]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # move input to same device as model\n",
    "        input_tensor = dataset.text_to_tokens(text).unsqueeze(0).to(device)\n",
    "        predicted_tokens = model.generate(input_tensor)\n",
    "        predicted_text = token_to_text(predicted_tokens)\n",
    "    \n",
    "    return \"\".join(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec31b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4637 [00:00<?, ?it/s]W0201 17:07:58.231000 53303 torch/_inductor/utils.py:1679] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "\n",
      "\n",
      "\n",
      "epoch: 1, step: 0, loss 474.83\n",
      "  1%|          | 45/4637 [00:32<46:16,  1.65it/s]                                                                                                                                                                                                                                                                 "
     ]
    }
   ],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, dataloader, device):\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # Compile the model (PyTorch 2.0)\n",
    "        compiled_model = torch.compile(model.to(device))\n",
    "        compiled_model.train()\n",
    "\n",
    "        optim = torch.optim.AdamW(compiled_model.parameters(), lr=5e-4)\n",
    "        num_epochs = 1\n",
    "\n",
    "        # Use GradScaler only for CUDA\n",
    "        use_amp = device.startswith(\"cuda\")\n",
    "        scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            pbar = tqdm(dataloader)\n",
    "            for idx, (x, y) in enumerate(pbar):\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                sliding_loss = 0\n",
    "\n",
    "                # Mixed precision if CUDA, else normal float\n",
    "                autocast_context = torch.autocast(device_type='cuda', dtype=torch.float16) if use_amp else torch.autocast(device_type=device)\n",
    "                with autocast_context:\n",
    "                    pred, loss = compiled_model(x, y)\n",
    "\n",
    "                if use_amp:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optim)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "\n",
    "                compiled_model.zero_grad(set_to_none=True)\n",
    "\n",
    "                sliding_loss += loss.detach().item()\n",
    "                if idx % 50 == 0:\n",
    "                    sliding_loss *= 100\n",
    "                    pbar.display(f'\\n\\n\\nepoch: {epoch+1}, step: {idx}, loss {sliding_loss:.2f}\\n'\n",
    "                                 + \"Generated text: \" + generate_text(\"To Sherlock Holmes\", compiled_model, dataloader.dataset))\n",
    "                    sliding_loss = 0\n",
    "\n",
    "        # Save model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': compiled_model.state_dict(),\n",
    "            'optimizer_state_dict': optim.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, \"my_gpt.pt\")\n",
    "\n",
    "\n",
    "# Usage:\n",
    "Trainer(gpt, dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4cb602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAUST. nsn sl  d,iecrteh ucnie eh sersieidegcna ehns t\n",
      "air inn emeheht l\n",
      "ann,\n",
      "sicsl za Gerrr ues erehen ahrhrnsan, ri di s tah is is ars icheneach t g.\n",
      "Schcha hakun leml sit d da ml\n",
      "eh  Stehnetde h\n",
      "anisat d Wehenn anehid\n",
      "Und,\n",
      "Dirabeicndemdeheh ain \n"
     ]
    }
   ],
   "source": [
    "loaded_model = VisionAndLanguageTransformer(context_size=256,\n",
    "                                     n_classes=dataset.vocab_size,\n",
    "                                     embed_dim = 384,\n",
    "                                    is_ViT=False,\n",
    "                                    depth=4).to(device)\n",
    "\n",
    "lm = torch.compile(loaded_model)\n",
    "checkpoint = torch.load(\"my_gpt.pt\")\n",
    "lm.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "lm.eval()\n",
    "text = \"To Sherlock Holmes\"\n",
    "pred_text = generate_text(text, lm, dataset)\n",
    "print(pred_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
